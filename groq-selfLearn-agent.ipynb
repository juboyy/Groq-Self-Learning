{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juboyy/Groq-Self-Learning/blob/main/groq-selfLearn-agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zwFnJsE6vjf8",
        "outputId": "7f5cd4b9-9486-4f48-a710-2fd4f43d5b2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully installed dataclasses-json-0.6.7 groq-0.11.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jiter-0.5.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.1 langchain-google-genai-2.0.0 langchain-groq-0.2.0 langchain-text-splitters-0.3.0 langchain_community-0.3.1 langchain_core-0.3.6 langchain_openai-0.2.1 langsmith-0.1.128 marshmallow-3.22.0 mypy-extensions-1.0.0 openai-1.49.0 orjson-3.10.7 pydantic-settings-2.5.2 python-dotenv-1.0.1 tenacity-8.5.0 tiktoken-0.7.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "# pip é como uma loja de aplicativos, mas em vez de baixar apps, ele baixa ferramentas e componentes que os programadores usam para construir programas.\n",
        "# !pip install é o comando que usamos para dizer ao Python: \"Baixe e instale essas ferramentas para mim, por favor!\"\n",
        "# Neste caso específico, estamos pedindo para instalar várias ferramentas relacionadas à criação de programas de Inteligência Artificial (IA) e automação, aqui é onde você instala seus modelos.\n",
        "\n",
        "!pip install langchain_openai langchain-google-genai langchain-groq langchain_core python-dotenv langchain langchain_community"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### **1. Importações de Ferramentas**\n",
        "**ChatGroq**: Estes são como \"conversadores de IA\" que podem responder a perguntas ou ajudar a realizar tarefas com inteligência artificial. Eles permitem conectar e conversar com sistemas de IA diferentes, como o ChatGPT (OpenAI) e o modelo Groq.\n",
        "\n",
        "  **`ChatMessageHistory`**: Mantém um histórico das conversas, como se fosse um diário de tudo o que foi discutido até agora com o \"conversador de IA\".\n",
        "\n",
        "  **`ChatPromptTemplate`** e **`MessagesPlaceholder`**: Estas são ferramentas para formatar mensagens e diálogos que você terá com a IA, ajudando a configurar como a conversa deve acontecer.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### **2. Carregar Variáveis de Configuração**\n",
        "`from dotenv import load_dotenv`: Esta ferramenta permite carregar informações confidenciais (como senhas ou chaves de acesso) de um arquivo seguro, para que o código possa usá-las sem expor esses dados diretamente.\n",
        "`os.environ[\"GROQ_API_KEY\"]`: Configura a chave de acesso para o sistema de IA Groq. Isso é como dizer à IA: \"Aqui está minha chave; agora podemos conversar.\"\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "uOaYxjMFo5o2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain.memory import ChatMessageHistory\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "load_dotenv()\n",
        "os.environ[\"GROQ_API_KEY\"] = os.getenv('GROQ_API_KEY')"
      ],
      "metadata": {
        "id": "ppOubv57fV-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Vamos definir funções de apoio para cada habilidade do nosso agente**"
      ],
      "metadata": {
        "id": "OMzaZih3f6sT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta função obtém o histórico de conversas para uma sessão específica.\n",
        "    \n",
        "Parâmetros:\n",
        "- store: Um dicionário que guarda os históricos de conversa para diferentes sessões.\n",
        "- session_id: Uma string que identifica de forma única uma sessão de usuário.\n",
        "    \n",
        "Retorna:\n",
        "- O histórico de mensagens associado ao session_id fornecido.\n",
        "  Se não existir um histórico para essa sessão, ele cria um novo."
      ],
      "metadata": {
        "id": "u2UgvHaZ7fud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gerenciamento de Histórico do chat\n",
        "\n",
        "def get_chat_history(store, session_id: str):\n",
        "    if session_id not in store:\n",
        "        store[session_id] = ChatMessageHistory()\n",
        "    return store[session_id]"
      ],
      "metadata": {
        "id": "f6NvTnAXgC1v"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Esta função gera uma resposta com base na entrada do usuário e em informações adicionais.\n",
        "\n",
        "Parâmetros:\n",
        "- chain_with_history: Um objeto que mantém o histórico da conversa.\n",
        "- human_input (str): A mensagem que o usuário enviou.\n",
        "- session_id (str): Um identificador único para acompanhar a sessão do usuário.\n",
        "- insights (str): Informações que ajudam a melhorar a resposta.\n",
        "\n",
        "Retorna:\n",
        "- A resposta gerada como uma string.\n"
      ],
      "metadata": {
        "id": "10haT6lT7oQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gerar Resposta\n",
        "\n",
        "def generate_response(chain_with_history, human_input: str, session_id: str, insights: str):\n",
        "    response = chain_with_history.invoke(\n",
        "        {\"input\": human_input, \"insights\": insights},\n",
        "        config={\"configurable\": {\"session_id\": session_id}}\n",
        "    )\n",
        "    return response.content"
      ],
      "metadata": {
        "id": "Ag9FZbyKgepp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta função analisa o histórico de conversas e fornece insights para melhorar as respostas.\n",
        "\n",
        "Parâmetros:\n",
        "- llm: Modelo de linguagem utilizado para gerar a reflexão.\n",
        "- store: Dicionário que armazena o histórico das conversas.\n",
        "- session_id: Identificador único da sessão.\n",
        "\n",
        "Retorna:\n",
        "- Conteúdo da reflexão gerada."
      ],
      "metadata": {
        "id": "pd0lluQV8n8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reflexão\n",
        "def reflect(llm, store, session_id: str):\n",
        "    reflection_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"Based on the following conversation history, provide insights on how to improve responses:\"),\n",
        "        MessagesPlaceholder(variable_name=\"history\"),\n",
        "        (\"human\", \"Generate insights for improvement:\")\n",
        "    ])\n",
        "    reflection_chain = reflection_prompt | llm\n",
        "    history = get_chat_history(store, session_id)\n",
        "    reflection_response = reflection_chain.invoke({\"history\": history.messages})\n",
        "    return reflection_response.content"
      ],
      "metadata": {
        "id": "PjhVSc-ngt7X"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta função ajuda o agente a aprender com base em insights fornecidos.\n",
        "Ela cria um prompt que instrui o agente a atualizar seu conhecimento e comportamento com base nesses insights,\n",
        "resume os 10 pontos-chave relevantes e realiza uma pesquisa básica sobre cada tópico.\n",
        "\n",
        "Em seguida, o agente adiciona uma mensagem ao histórico da conversa indicando o que foi aprendido."
      ],
      "metadata": {
        "id": "nYPJRidd8tYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aprendizado\n",
        "def learn(llm, store, session_id: str, insights: str):\n",
        "    learning_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"Based on these insights, update the agent's knowledge and behavior:\"),\n",
        "        (\"human\", \"{insights}\"),\n",
        "        (\"human\", \"Summarize the 10 relevant key points to remember and provide a basic research about each topic:\")\n",
        "    ])\n",
        "    learning_chain = learning_prompt | llm\n",
        "    learned_points = learning_chain.invoke({\"insights\": insights}).content\n",
        "    get_chat_history(store, session_id).add_ai_message(f\"[SYSTEM] Agent learned: {learned_points}\")\n",
        "    return learned_points"
      ],
      "metadata": {
        "id": "Efpsb3X_hBKs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " A classe SelfImprovingAgent é uma implementação de um agente de IA auto-aprimorável em Python.\n",
        " Ela utiliza um modelo de linguagem (LLM) da Groq para processar e gerar respostas.\n",
        "\n",
        " A classe é inicializada com um LLM, um dicionário para armazenar o histórico de conversas e uma string para insights.\n",
        "\n",
        " O prompt do agente é configurado para aprender e melhorar ao longo do tempo, respondendo em português.\n",
        "\n",
        " A classe usa um sistema de cadeia (chain) para processar as entradas, que inclui o histórico da conversa.\n",
        "\n",
        " Os métodos principais são: 'respond' para gerar respostas, 'reflect' para analisar o histórico e gerar insights, e 'learn' para aplicar esses insights e melhorar o desempenho do agente.\n",
        "\n",
        " Esta estrutura permite que o agente melhore continuamente suas respostas com base em interações anteriores.\n"
      ],
      "metadata": {
        "id": "YJ9zfG_I84O2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Auto-aprendizado Agent Class\n",
        "# vamos definir nossa classe de agente que utiliza todas estas funções acima\n",
        "\n",
        "class SelfImprovingAgent:\n",
        "    def __init__(self):\n",
        "        self.llm = ChatGroq(model=\"llama3-groq-70b-8192-tool-use-preview\", max_tokens=2000, temperature=0.7)\n",
        "        self.store = {}\n",
        "        self.insights = \"\"\n",
        "\n",
        "        self.prompt = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"You are a self-improving AI assistant. Learn from your interactions and improve your performance over time. apenas responda em pt-br\"),\n",
        "            MessagesPlaceholder(variable_name=\"history\"),\n",
        "            (\"human\", \"{input}\"),\n",
        "            (\"system\", \"Recent insights for response improvement: {insights}\")\n",
        "        ])\n",
        "\n",
        "        self.chain = self.prompt | self.llm\n",
        "        self.chain_with_history = RunnableWithMessageHistory(\n",
        "            self.chain,\n",
        "            lambda session_id: get_chat_history(self.store, session_id),\n",
        "            input_messages_key=\"input\",\n",
        "            history_messages_key=\"history\"\n",
        "        )\n",
        "\n",
        "    def respond(self, human_input: str, session_id: str):\n",
        "        return generate_response(self.chain_with_history, human_input, session_id, self.insights)\n",
        "\n",
        "    def reflect(self, session_id: str):\n",
        "        self.insights = reflect(self.llm, self.store, session_id)\n",
        "        return self.insights\n",
        "\n",
        "    def learn(self, session_id: str):\n",
        "        self.reflect(session_id)\n",
        "        return learn(self.llm, self.store, session_id, self.insights)\n"
      ],
      "metadata": {
        "id": "VTAQmajNhEj_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Hora de testar!**"
      ],
      "metadata": {
        "id": "dhU-0SGllwGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = SelfImprovingAgent()\n",
        "session_id = \"user_123\"\n",
        "\n",
        "\n",
        "# 1 interação\n",
        "print(\"AI:\", agent.respond(\"What's the capital of Brazil?\", session_id))\n",
        "\n",
        "# 2 interação\n",
        "print(\"AI:\", agent.respond(\"Can you tell me more about its history?\", session_id))\n",
        "\n",
        "# interação Learn and improve\n",
        "print(\"\\nReflecting and learning...\")\n",
        "learned = agent.learn(session_id)\n",
        "print(\"Learned:\", learned)\n",
        "\n",
        "#4 interação (to demonstrate continued improvement)\n",
        "print(\"AI:\", agent.respond(\"What's another interesting fact about this city?\", session_id))\n",
        "\n",
        "#5 interação (to demonstrate continued improvement)\n",
        "print(\"AI:\", agent.respond(\"me diga as maiores curiosidades?\", session_id))\n",
        "\n",
        "# interação Learn and improve\n",
        "print(\"\\nReflecting and learning...\")\n",
        "learned = agent.learn(session_id)\n",
        "print(\"Learned:\", learned)\n",
        "\n",
        "#3 interação (potentially improved based on learning)\n",
        "print(\"\\nAI:\", agent.respond(\"qual a capital do brasil?\", session_id))"
      ],
      "metadata": {
        "id": "-wfFSUaDhm7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fDyP0SKaho78"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Olá, este é o Colaboratory",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}